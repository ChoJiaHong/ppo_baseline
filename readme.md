---

# ğŸš€ **PPO åŸºç¤ä»£ç¢¼å·²å®Œæˆ**

æœ¬å°ˆæ¡ˆå·²åŒ…å«å®Œæ•´çš„ PPO (Proximal Policy Optimization) åŸºç¤å¯¦ä½œä»£ç¢¼ï¼

ğŸ“‚ **å¿«é€Ÿé–‹å§‹ï¼š**
- æŸ¥çœ‹ [README_PPO.md](README_PPO.md) äº†è§£å®Œæ•´ä½¿ç”¨èªªæ˜
- åŸ·è¡Œ `python demo.py` æŸ¥çœ‹å¿«é€Ÿç¤ºç¯„
- åŸ·è¡Œ `python train.py` é–‹å§‹è¨“ç·´
- åŸ·è¡Œ `python evaluate.py --checkpoint_path checkpoints/ppo_final.pt` è©•ä¼°è¨“ç·´å¥½çš„æ¨¡å‹

ğŸ“¦ **åŒ…å«çš„æ¨¡çµ„ï¼š**
- `environment.py` - æœå‹™éƒ¨ç½²ç’°å¢ƒ (Gymnasium)
- `policy.py` - Actor-Critic ç­–ç•¥ç¶²è·¯
- `buffer.py` - ç¶“é©—å›æ”¾ç·©è¡å€
- `ppo_trainer.py` - PPO è¨“ç·´å™¨
- `train.py` - ä¸»è¨“ç·´è…³æœ¬
- `evaluate.py` - è©•ä¼°è…³æœ¬
- `demo.py` - å¿«é€Ÿç¤ºç¯„è…³æœ¬
- `config.yaml` - é…ç½®æª”æ¡ˆ

---

# ğŸ§© **ï¼ˆAï¼‰å•é¡ŒèƒŒæ™¯èˆ‡ç›®æ¨™**

ä½ æ­£åœ¨åšï¼š

**å¤šæœå‹™ï¼ˆmulti-serviceï¼‰ï¼‹å¤šç¯€é»ï¼ˆedge nodesï¼‰ç’°å¢ƒä¸‹çš„å‹•æ…‹æœå‹™éƒ¨ç½²ï¼é »ç‡åˆ†é…ï¼ˆper-second throughput allocationï¼‰å•é¡Œã€‚**

ç’°å¢ƒä¸­æœƒå‹•æ…‹ç™¼ç”Ÿäº‹ä»¶ï¼š

* agent arrivalï¼ˆæ–°å¢ä½¿ç”¨è€…ï¼‰
* agent departureï¼ˆé›¢é–‹ï¼‰
* node failureï¼ˆç¯€é»æ•…éšœï¼‰
* node recoveryï¼ˆç¯€é»æ¢å¾©ï¼‰
* no_eventï¼ˆç„¡äº‹ä»¶ï¼‰

ä½ çš„ RL agent å¿…é ˆåœ¨æ¯æ¬¡äº‹ä»¶ç™¼ç”Ÿæ™‚åšæ±ºç­–ï¼š

* è¦æŠŠæœå‹™éƒ¨ç½²åœ¨å“ªå€‹ nodeï¼Ÿ
* è¦çµ¦é€™äº› agent åˆ†é…å¤šå°‘ throughputï¼ˆæ¯ç§’å¼µæ•¸ï¼‰ï¼Ÿ
* è¦ä¸è¦é‡æ–°éƒ¨ç½²åˆªé™¤ï¼æ’¤å›æŸäº›æœå‹™ï¼Ÿ
* æ˜¯å¦éœ€è¦é¿å… co-locationï¼ˆæœå‹™äº’ç›¸å¹²æ“¾ï¼‰

ç›®æ¨™æ˜¯ï¼š

* **æœ€å¤§åŒ– agent çš„æˆåŠŸç‡ï¼æœå‹™å“è³ª**
* **æ»¿è¶³ W è¡¨å®šç¾©çš„ååèƒ½åŠ›**
* **é¿å… QoS violation**
* **åœ¨äº‹ä»¶åºåˆ—ä¸­é•·æœŸä¿æŒå¯è¡Œæ€§ï¼ˆfeasibilityï¼‰**

---

# ğŸ§© **ï¼ˆBï¼‰W è¡¨ï¼ˆthroughput tableï¼‰çš„æœ¬è³ªèˆ‡ç”¨é€”**

ä½ æœ‰ä¸€å€‹ W è¡¨ï¼Œè£¡é¢è¨˜è¼‰ï¼š

```
W[node, service_set] = åœ¨ç‰¹å®š node åŒæ™‚éƒ¨ç½²ä¸€çµ„æœå‹™æ™‚ï¼ŒæŸæœå‹™èƒ½é”åˆ°çš„ç©©å®šæœ€å¤§ throughput
```

W è¡¨ç”¨é€”ï¼š

* é æ¸¬ co-location é€ æˆçš„æ•ˆèƒ½ä¸‹é™
* æ±ºå®šæŸ node æ˜¯å¦èƒ½æ‰¿è¼‰æ–° agent çš„éœ€æ±‚
* å®šç¾©æ•´å€‹ç³»çµ±çš„ **å¯è¡Œè§£ç©ºé–“ï¼ˆfeasible regionï¼‰**

W è¡¨éå¸¸é‡è¦ï¼Œå› ç‚ºï¼š

* å…è¨± RL åšæ›´ç²¾æº–çš„ QoS æ§åˆ¶
* è®“ constrained RL è®Šå¾—å¯èƒ½
* è®“ reward / constraint éƒ½å¯ä»¥æ•¸å­¸åŒ–

ä¹Ÿæ²’æœ‰é•å RL ç²¾ç¥ï¼Œå› ç‚º W è¡¨åªæ˜¯**ç’°å¢ƒæ¨¡å‹çš„ä¸€éƒ¨åˆ†**ã€‚

---

# ğŸ§© **ï¼ˆCï¼‰ç’°å¢ƒï¼ˆGym-likeï¼‰çš„æœ€çµ‚é¸æ“‡**

### âœ” Observationï¼ˆç‹€æ…‹ï¼‰é¸æ“‡ s1

åŒ…å«ï¼š

* ç•¶å‰ event typeï¼ˆone-hotï¼‰
* å„ nodeçš„æœå‹™ occupancy
* å„ node çš„ W capacityï¼ˆæˆ– normalized throughputï¼‰
* agent characteristicsï¼ˆéœ€æ±‚é »ç‡ï¼‰
* node ç‹€æ…‹ï¼ˆæ­£å¸¸ï¼æ•…éšœï¼‰

### âœ” Action spaceï¼ˆg1ï¼‰

ç°¡åŒ–ç‰ˆ action spaceï¼š

```
1. éƒ¨ç½²åˆ° node 1
2. éƒ¨ç½²åˆ° node 2
3. éƒ¨ç½²åˆ° node 3
4. æ‹’çµ•ï¼ç„¡å‹•ä½œ
```

æœªä¾†å¯æ“´å±•ï¼Œä½†ç¾åœ¨å…ˆå°ï¼Œä»¥ä¾¿è®“æ•´å€‹ç³»çµ±è·‘èµ·ä¾†ã€‚

### âœ” Rewardï¼ˆR1ï¼‰

ä¸» rewardï¼š

```
+1 æˆåŠŸéƒ¨ç½² / æˆåŠŸç¶­æŒ QoS
-1 ç„¡æ³•æ»¿è¶³ QoSï¼ˆW è¡¨ overloadï¼‰
-2 node failure ä¸”è©² node æœ‰æœå‹™
```

ç¬¬äºŒéšæ®µï¼ˆtie-breakï¼‰ç›®æ¨™ï¼š

* é¿å…ç©ºè¼‰éƒ¨ç½²
* é¿å…ä¸å¿…è¦çš„ redeploy

---

# ğŸ§© **ï¼ˆDï¼‰ä½ çš„ç ”ç©¶æœ¬è³ª = ä¸€å€‹ CMDPï¼ˆConstrained MDPï¼‰**

ä½ çš„å•é¡Œä¸æ˜¯ä¸€èˆ¬ RLï¼Œè€Œæ˜¯ï¼š

# **ä¸€å€‹å¤šé‡ç¡¬æ€§é™åˆ¶çš„ CMDPï¼ˆConstrained MDPï¼‰**

é™åˆ¶ä¾†è‡ªï¼š

* W è¡¨ throughput ä¸Šé™
* co-location å¹²æ“¾
* node failure å°è‡´è³‡æºæ¶ˆå¤±
* minimal QoS per agentï¼ˆé »ç‡ä¸‹é™ï¼‰
* æœå‹™éƒ¨ç½²çµ„åˆï¼ˆcombinatorial constraintï¼‰

é€™äº›é™åˆ¶ï¼š

* ä¸å¯å¾®
* ä¸é€£çºŒ
* éå‡¸
* é«˜åº¦çµ„åˆæ€§

å› æ­¤ï¼š

# â­ å¿…é ˆä½¿ç”¨ Constrained RLï¼ˆCï¼‰â†’ å¾ˆé©åˆåšç ”ç©¶è²¢ç»

# â­ çµ•ä¸æ˜¯ã€Œé•å RL ç²¾ç¥ã€

---

# ğŸ§© **ï¼ˆEï¼‰ç‚ºä½•ä½¿ç”¨ Constrained RLï¼Ÿ3 å¤§æ ¸å¿ƒç›®çš„**

1ï¸âƒ£ **é¿å… RL æ¢ç´¢ä¸å¯è¡Œè§£ï¼ˆfeasible explorationï¼‰**
W è¡¨çš„é™åˆ¶éå¸¸ç¡¬ï¼Œæ™®é€š PPO æœƒä¸åœè©¦ infeasible action â†’ reward ä¸€ç›´è² å€¼ â†’ ç„¡æ³•å­¸ç¿’ã€‚

2ï¸âƒ£ **åœ¨éå‡¸ã€ä¸è¦å‰‡çš„å¯è¡ŒåŸŸä¸­å­¸ç¿’æœ€ä½³ç­–ç•¥**
W è¡¨å½¢æˆéå¸¸è¤‡é›œçš„ feasible regionï¼Œåªæœ‰ constrained RL èƒ½è™•ç†ã€‚

3ï¸âƒ£ **è‡ªå‹•å¹³è¡¡ reward èˆ‡ constraintï¼ˆdual updateï¼‰**
Lagrangian:

```
L = reward - Î» * violation
Î» â† Î» + Î· * violation
```

é€™è®“ RL è‡ªå‹•å­¸ã€Œé«˜ performance ä¸”ä¸é•è¦ã€çš„ç­–ç•¥ã€‚

---

# ğŸ§© **ï¼ˆFï¼‰ä½ æœªä¾†æ‰€æœ‰å‰µæ–°å¯åˆ‡å…¥çš„ä½ç½®ï¼ˆç ”ç©¶ç©ºé–“ï¼‰**

ä½ æœ€é‡è¦çš„ç ”ç©¶å‡ºå£ï¼š

### 1. **Constraint Modelingï¼ˆæ ¸å¿ƒç ”ç©¶é»ï¼‰**

* æ€éº¼æŠŠ W è¡¨å¯«æˆ constraintï¼Ÿ
* æ€éº¼åº¦é‡ violationï¼Ÿ

### 2. **Reward Shaping**

* åŸºæ–¼ W è¡¨çš„ QoS penalty
* smooth penalty vs hard penalty

### 3. **Lagrangian Structure (dual variables)**

* å¤š constraint å¦‚ä½•æ›´æ–° Î»ï¼Ÿ
* æ˜¯å¦éœ€è¦ per-node Î»ï¼Ÿ

### 4. **Feasible Action Projection**

* PPO è¼¸å‡º action â†’ æŠ•å½±åˆ°æœ€è¿‘ feasible actionï¼ˆä½ çš„æ¶æ§‹å‰µæ–°é»ï¼‰

### 5. **Observation Encoding**

* å¦‚ä½•æŠŠ W è¡¨ encode é€² stateï¼Ÿ
* ç”¨ MLPï¼ŸGraphï¼ŸTransformerï¼Ÿ

### 6. **Policy Architecture**

* æ˜¯å¦åŠ å…¥ attentionï¼ˆçœ‹å“ªäº› node å®¹é‡æœ€é—œéµï¼‰ï¼Ÿ

é€™äº›éƒ½æ˜¯å¯ä»¥ç•¶è«–æ–‡çš„å‰µæ–°ã€‚

---

# ğŸ§© **ï¼ˆGï¼‰ä½ è¦å¦‚ä½•é–‹å§‹ç ”ç©¶ï¼ˆæ˜ç¢ºæ­¥é©Ÿï¼‰**

R0 â†’ R6ï¼š

### **R0ï¼šå›ºå®š baselineï¼ˆPPO without constraintsï¼‰**

ä½ å·²å®Œæˆã€‚

### **R1ï¼šå»ºç«‹ baseline performance map**

æª¢æŸ¥ PPO ä½•æ™‚å¤±æ•—ï¼ä½•æ™‚æˆåŠŸã€‚

### **R2ï¼šæ‰¾å‡ºå•é¡Œç—›é»**

ä¸»è¦åŒ…æ‹¬ï¼š

* æ¢ç´¢å¤ªå¤š infeasible action
* ç„¡æ³•æ§åˆ¶ QoS violation
* ç„¡æ³•ç†è§£ W è¡¨

é€™å°±æ˜¯ä½ ç”¨ C çš„ç†ç”±ã€‚

### **R3ï¼šåŠ ä¸Š Constrained RLï¼ˆä½ çš„ä¸»è¦ç ”ç©¶ï¼‰**

ä¸‰æ¢è·¯ç·šï¼š

* C1ï¼šLagrangian PPOï¼ˆæœ€å­¸è¡“ï¼‰
* C2ï¼šFeasible action projectionï¼ˆå¾ˆæ–°ç©ï¼‰
* C3ï¼šW-aware reward shapingï¼ˆæœ€å®¹æ˜“å…ˆåšï¼‰

### **R4ï¼šæ¨¡çµ„åŒ–ä½ çš„æ¶æ§‹**

é¿å…ææ··æ¯æ¬¡æ”¹å“ªè£¡ã€‚

### **R5ï¼šåš Ablation Study**

æ¯”è¼ƒï¼š

* PPO baseline
* PPO + penalty
* PPO + projection
* Lagrangian PPO
* Full model
